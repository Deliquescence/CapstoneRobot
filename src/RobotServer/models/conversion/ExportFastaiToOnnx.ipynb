{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "import torch.nn.functional as F\n",
    "from fastai.layers import *\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"supervised\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From fastai\n",
    "def conv_bn_lrelu(ni:int, nf:int, ks:int=3, stride:int=1)->nn.Sequential:\n",
    "    \"Create a seuence Conv2d->BatchNorm2d->LeakyReLu layer.\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ni, nf, kernel_size=ks, bias=False, stride=stride, padding=ks//2),\n",
    "        nn.BatchNorm2d(nf),\n",
    "        nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "\n",
    "class ResLayer(nn.Module):\n",
    "    \"Resnet style layer with `ni` inputs.\"\n",
    "    def __init__(self, ni:int):\n",
    "        super().__init__()\n",
    "        self.conv1=conv_bn_lrelu(ni, ni//2, ks=1)\n",
    "        self.conv2=conv_bn_lrelu(ni//2, ni, ks=3)\n",
    "\n",
    "    def forward(self, x): return x + self.conv2(self.conv1(x))\n",
    "\n",
    "# From fastai, modified head\n",
    "class CustomDarknet(nn.Module):\n",
    "    \"https://github.com/pjreddie/darknet\"\n",
    "\n",
    "    def make_group_layer(self, ch_in: int, num_blocks: int, stride: int = 1):\n",
    "        \"starts with conv layer - `ch_in` channels in - then has `num_blocks` `ResLayer`\"\n",
    "        return [conv_bn_lrelu(ch_in, ch_in * 2, stride=stride)\n",
    "                ] + [(ResLayer(ch_in * 2)) for i in range(num_blocks)]\n",
    "\n",
    "    def __init__(self, num_blocks: Collection[int], num_classes: int, nf=32):\n",
    "        \"create darknet with `nf` and `num_blocks` layers\"\n",
    "        super().__init__()\n",
    "        layers = [conv_bn_lrelu(3, nf, ks=3, stride=1)]\n",
    "        for i, nb in enumerate(num_blocks):\n",
    "            layers += self.make_group_layer(nf, nb, stride=2 - (i == 1))\n",
    "            nf *= 2\n",
    "        layers += [nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(nf, num_classes)]\n",
    "        layers += [nn.Linear(num_classes, 2), SigmoidRange(-1, 1)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method model_summary of Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (0 items)\n",
       "x: ImageItemList\n",
       "\n",
       "y: FloatList\n",
       "\n",
       "Path: ..;\n",
       "\n",
       "Valid: LabelList (0 items)\n",
       "x: ImageItemList\n",
       "\n",
       "y: FloatList\n",
       "\n",
       "Path: ..;\n",
       "\n",
       "Test: None, model=CustomDarknet(\n",
       "  (layers): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (2): ResLayer(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (4): ResLayer(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "      )\n",
       "    )\n",
       "    (5): ResLayer(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (7): ResLayer(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=1)\n",
       "    (9): Flatten()\n",
       "    (10): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (11): Linear(in_features=10, out_features=2, bias=True)\n",
       "    (12): SigmoidRange()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of MSELoss(), metrics=[], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=WindowsPath('..'), model_dir='models', callback_fns=[<class 'fastai.basic_train.Recorder'>], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (6): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (9): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (15): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (16): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (18): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (21): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (22): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (23): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (24): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (26): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (27): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (29): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (30): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (32): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (33): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (34): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (35): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (36): AdaptiveAvgPool2d(output_size=1)\n",
       "  (37): Flatten()\n",
       "  (38): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (39): Linear(in_features=10, out_features=2, bias=True)\n",
       "  (40): SigmoidRange()\n",
       ")])>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = load_learner(\"../\", fname=\"%s.pkl\" % model_name)\n",
    "learn.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = learn.model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%0 : Float(1, 3, 240, 320)\n",
      "      %1 : Float(16, 3, 3, 3)\n",
      "      %2 : Float(16)\n",
      "      %3 : Float(16)\n",
      "      %4 : Float(16)\n",
      "      %5 : Float(16)\n",
      "      %6 : Long()\n",
      "      %7 : Float(32, 16, 3, 3)\n",
      "      %8 : Float(32)\n",
      "      %9 : Float(32)\n",
      "      %10 : Float(32)\n",
      "      %11 : Float(32)\n",
      "      %12 : Long()\n",
      "      %13 : Float(16, 32, 1, 1)\n",
      "      %14 : Float(16)\n",
      "      %15 : Float(16)\n",
      "      %16 : Float(16)\n",
      "      %17 : Float(16)\n",
      "      %18 : Long()\n",
      "      %19 : Float(32, 16, 3, 3)\n",
      "      %20 : Float(32)\n",
      "      %21 : Float(32)\n",
      "      %22 : Float(32)\n",
      "      %23 : Float(32)\n",
      "      %24 : Long()\n",
      "      %25 : Float(64, 32, 3, 3)\n",
      "      %26 : Float(64)\n",
      "      %27 : Float(64)\n",
      "      %28 : Float(64)\n",
      "      %29 : Float(64)\n",
      "      %30 : Long()\n",
      "      %31 : Float(32, 64, 1, 1)\n",
      "      %32 : Float(32)\n",
      "      %33 : Float(32)\n",
      "      %34 : Float(32)\n",
      "      %35 : Float(32)\n",
      "      %36 : Long()\n",
      "      %37 : Float(64, 32, 3, 3)\n",
      "      %38 : Float(64)\n",
      "      %39 : Float(64)\n",
      "      %40 : Float(64)\n",
      "      %41 : Float(64)\n",
      "      %42 : Long()\n",
      "      %43 : Float(32, 64, 1, 1)\n",
      "      %44 : Float(32)\n",
      "      %45 : Float(32)\n",
      "      %46 : Float(32)\n",
      "      %47 : Float(32)\n",
      "      %48 : Long()\n",
      "      %49 : Float(64, 32, 3, 3)\n",
      "      %50 : Float(64)\n",
      "      %51 : Float(64)\n",
      "      %52 : Float(64)\n",
      "      %53 : Float(64)\n",
      "      %54 : Long()\n",
      "      %55 : Float(128, 64, 3, 3)\n",
      "      %56 : Float(128)\n",
      "      %57 : Float(128)\n",
      "      %58 : Float(128)\n",
      "      %59 : Float(128)\n",
      "      %60 : Long()\n",
      "      %61 : Float(64, 128, 1, 1)\n",
      "      %62 : Float(64)\n",
      "      %63 : Float(64)\n",
      "      %64 : Float(64)\n",
      "      %65 : Float(64)\n",
      "      %66 : Long()\n",
      "      %67 : Float(128, 64, 3, 3)\n",
      "      %68 : Float(128)\n",
      "      %69 : Float(128)\n",
      "      %70 : Float(128)\n",
      "      %71 : Float(128)\n",
      "      %72 : Long()\n",
      "      %73 : Float(10, 128)\n",
      "      %74 : Float(10)\n",
      "      %75 : Float(2, 10)\n",
      "      %76 : Float(2)) {\n",
      "  %77 : Float(1, 16, 240, 320) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%0, %1), scope: CustomDarknet/Sequential[layers]/Sequential[0]/Conv2d[0]\n",
      "  %78 : Float(1, 16, 240, 320) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%77, %2, %3, %4, %5), scope: CustomDarknet/Sequential[layers]/Sequential[0]/BatchNorm2d[1]\n",
      "  %79 : Float(1, 16, 240, 320) = onnx::LeakyRelu[alpha=0.1](%78), scope: CustomDarknet/Sequential[layers]/Sequential[0]/LeakyReLU[2]\n",
      "  %80 : Float(1, 32, 120, 160) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%79, %7), scope: CustomDarknet/Sequential[layers]/Sequential[1]/Conv2d[0]\n",
      "  %81 : Float(1, 32, 120, 160) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%80, %8, %9, %10, %11), scope: CustomDarknet/Sequential[layers]/Sequential[1]/BatchNorm2d[1]\n",
      "  %82 : Float(1, 32, 120, 160) = onnx::LeakyRelu[alpha=0.1](%81), scope: CustomDarknet/Sequential[layers]/Sequential[1]/LeakyReLU[2]\n",
      "  %83 : Float(1, 16, 120, 160) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%82, %13), scope: CustomDarknet/Sequential[layers]/ResLayer[2]/Sequential[conv1]/Conv2d[0]\n",
      "  %84 : Float(1, 16, 120, 160) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%83, %14, %15, %16, %17), scope: CustomDarknet/Sequential[layers]/ResLayer[2]/Sequential[conv1]/BatchNorm2d[1]\n",
      "  %85 : Float(1, 16, 120, 160) = onnx::LeakyRelu[alpha=0.1](%84), scope: CustomDarknet/Sequential[layers]/ResLayer[2]/Sequential[conv1]/LeakyReLU[2]\n",
      "  %86 : Float(1, 32, 120, 160) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%85, %19), scope: CustomDarknet/Sequential[layers]/ResLayer[2]/Sequential[conv2]/Conv2d[0]\n",
      "  %87 : Float(1, 32, 120, 160) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%86, %20, %21, %22, %23), scope: CustomDarknet/Sequential[layers]/ResLayer[2]/Sequential[conv2]/BatchNorm2d[1]\n",
      "  %88 : Float(1, 32, 120, 160) = onnx::LeakyRelu[alpha=0.1](%87), scope: CustomDarknet/Sequential[layers]/ResLayer[2]/Sequential[conv2]/LeakyReLU[2]\n",
      "  %89 : Float(1, 32, 120, 160) = onnx::Add(%82, %88), scope: CustomDarknet/Sequential[layers]/ResLayer[2]\n",
      "  %90 : Float(1, 64, 120, 160) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%89, %25), scope: CustomDarknet/Sequential[layers]/Sequential[3]/Conv2d[0]\n",
      "  %91 : Float(1, 64, 120, 160) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%90, %26, %27, %28, %29), scope: CustomDarknet/Sequential[layers]/Sequential[3]/BatchNorm2d[1]\n",
      "  %92 : Float(1, 64, 120, 160) = onnx::LeakyRelu[alpha=0.1](%91), scope: CustomDarknet/Sequential[layers]/Sequential[3]/LeakyReLU[2]\n",
      "  %93 : Float(1, 32, 120, 160) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%92, %31), scope: CustomDarknet/Sequential[layers]/ResLayer[4]/Sequential[conv1]/Conv2d[0]\n",
      "  %94 : Float(1, 32, 120, 160) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%93, %32, %33, %34, %35), scope: CustomDarknet/Sequential[layers]/ResLayer[4]/Sequential[conv1]/BatchNorm2d[1]\n",
      "  %95 : Float(1, 32, 120, 160) = onnx::LeakyRelu[alpha=0.1](%94), scope: CustomDarknet/Sequential[layers]/ResLayer[4]/Sequential[conv1]/LeakyReLU[2]\n",
      "  %96 : Float(1, 64, 120, 160) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%95, %37), scope: CustomDarknet/Sequential[layers]/ResLayer[4]/Sequential[conv2]/Conv2d[0]\n",
      "  %97 : Float(1, 64, 120, 160) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%96, %38, %39, %40, %41), scope: CustomDarknet/Sequential[layers]/ResLayer[4]/Sequential[conv2]/BatchNorm2d[1]\n",
      "  %98 : Float(1, 64, 120, 160) = onnx::LeakyRelu[alpha=0.1](%97), scope: CustomDarknet/Sequential[layers]/ResLayer[4]/Sequential[conv2]/LeakyReLU[2]\n",
      "  %99 : Float(1, 64, 120, 160) = onnx::Add(%92, %98), scope: CustomDarknet/Sequential[layers]/ResLayer[4]\n",
      "  %100 : Float(1, 32, 120, 160) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%99, %43), scope: CustomDarknet/Sequential[layers]/ResLayer[5]/Sequential[conv1]/Conv2d[0]\n",
      "  %101 : Float(1, 32, 120, 160) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%100, %44, %45, %46, %47), scope: CustomDarknet/Sequential[layers]/ResLayer[5]/Sequential[conv1]/BatchNorm2d[1]\n",
      "  %102 : Float(1, 32, 120, 160) = onnx::LeakyRelu[alpha=0.1](%101), scope: CustomDarknet/Sequential[layers]/ResLayer[5]/Sequential[conv1]/LeakyReLU[2]\n",
      "  %103 : Float(1, 64, 120, 160) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%102, %49), scope: CustomDarknet/Sequential[layers]/ResLayer[5]/Sequential[conv2]/Conv2d[0]\n",
      "  %104 : Float(1, 64, 120, 160) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%103, %50, %51, %52, %53), scope: CustomDarknet/Sequential[layers]/ResLayer[5]/Sequential[conv2]/BatchNorm2d[1]\n",
      "  %105 : Float(1, 64, 120, 160) = onnx::LeakyRelu[alpha=0.1](%104), scope: CustomDarknet/Sequential[layers]/ResLayer[5]/Sequential[conv2]/LeakyReLU[2]\n",
      "  %106 : Float(1, 64, 120, 160) = onnx::Add(%99, %105), scope: CustomDarknet/Sequential[layers]/ResLayer[5]\n",
      "  %107 : Float(1, 128, 60, 80) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%106, %55), scope: CustomDarknet/Sequential[layers]/Sequential[6]/Conv2d[0]\n",
      "  %108 : Float(1, 128, 60, 80) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%107, %56, %57, %58, %59), scope: CustomDarknet/Sequential[layers]/Sequential[6]/BatchNorm2d[1]\n",
      "  %109 : Float(1, 128, 60, 80) = onnx::LeakyRelu[alpha=0.1](%108), scope: CustomDarknet/Sequential[layers]/Sequential[6]/LeakyReLU[2]\n",
      "  %110 : Float(1, 64, 60, 80) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%109, %61), scope: CustomDarknet/Sequential[layers]/ResLayer[7]/Sequential[conv1]/Conv2d[0]\n",
      "  %111 : Float(1, 64, 60, 80) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%110, %62, %63, %64, %65), scope: CustomDarknet/Sequential[layers]/ResLayer[7]/Sequential[conv1]/BatchNorm2d[1]\n",
      "  %112 : Float(1, 64, 60, 80) = onnx::LeakyRelu[alpha=0.1](%111), scope: CustomDarknet/Sequential[layers]/ResLayer[7]/Sequential[conv1]/LeakyReLU[2]\n",
      "  %113 : Float(1, 128, 60, 80) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%112, %67), scope: CustomDarknet/Sequential[layers]/ResLayer[7]/Sequential[conv2]/Conv2d[0]\n",
      "  %114 : Float(1, 128, 60, 80) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%113, %68, %69, %70, %71), scope: CustomDarknet/Sequential[layers]/ResLayer[7]/Sequential[conv2]/BatchNorm2d[1]\n",
      "  %115 : Float(1, 128, 60, 80) = onnx::LeakyRelu[alpha=0.1](%114), scope: CustomDarknet/Sequential[layers]/ResLayer[7]/Sequential[conv2]/LeakyReLU[2]\n",
      "  %116 : Float(1, 128, 60, 80) = onnx::Add(%109, %115), scope: CustomDarknet/Sequential[layers]/ResLayer[7]\n",
      "  %117 : Float(1, 128, 1, 1) = onnx::GlobalAveragePool(%116), scope: CustomDarknet/Sequential[layers]/AdaptiveAvgPool2d[8]\n",
      "  %118 : Long() = onnx::Constant[value={0}](), scope: CustomDarknet/Sequential[layers]/Flatten[9]\n",
      "  %119 : Tensor = onnx::Shape(%117), scope: CustomDarknet/Sequential[layers]/Flatten[9]\n",
      "  %120 : Long() = onnx::Gather[axis=0](%119, %118), scope: CustomDarknet/Sequential[layers]/Flatten[9]\n",
      "  %121 : Long() = onnx::Constant[value={-1}](), scope: CustomDarknet/Sequential[layers]/Flatten[9]\n",
      "  %122 : Tensor = onnx::Unsqueeze[axes=[0]](%120)\n",
      "  %123 : Tensor = onnx::Unsqueeze[axes=[0]](%121)\n",
      "  %124 : Tensor = onnx::Concat[axis=0](%122, %123)\n",
      "  %125 : Float(1, 128) = onnx::Reshape(%117, %124), scope: CustomDarknet/Sequential[layers]/Flatten[9]\n",
      "  %126 : Float(1, 10) = onnx::Gemm[alpha=1, beta=1, transB=1](%125, %73, %74), scope: CustomDarknet/Sequential[layers]/Flatten[9]\n",
      "  %127 : Float(1, 2) = onnx::Gemm[alpha=1, beta=1, transB=1](%126, %75, %76), scope: CustomDarknet/Sequential[layers]/Flatten[9]\n",
      "  %128 : Float(1, 2) = onnx::Sigmoid(%127), scope: CustomDarknet/Sequential[layers]/SigmoidRange[12]\n",
      "  %129 : Tensor = onnx::Constant[value={2}]()\n",
      "  %130 : Tensor = onnx::Mul(%128, %129)\n",
      "  %131 : Tensor = onnx::Constant[value={-1}]()\n",
      "  %132 : Float(1, 2) = onnx::Add(%130, %131)\n",
      "  return (%132);\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 240, 320).cpu()\n",
    "torch.onnx.export(model, dummy_input, f=\"../%s.onnx\" % model_name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_model = onnx.load(\"../%s.onnx\" % model_name)\n",
    "onnx.checker.check_model(exported_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch-jit-export (\n",
      "  %0[FLOAT, 1x3x240x320]\n",
      ") initializers (\n",
      "  %1[FLOAT, 16x3x3x3]\n",
      "  %2[FLOAT, 16]\n",
      "  %3[FLOAT, 16]\n",
      "  %4[FLOAT, 16]\n",
      "  %5[FLOAT, 16]\n",
      "  %6[INT64, scalar]\n",
      "  %7[FLOAT, 32x16x3x3]\n",
      "  %8[FLOAT, 32]\n",
      "  %9[FLOAT, 32]\n",
      "  %10[FLOAT, 32]\n",
      "  %11[FLOAT, 32]\n",
      "  %12[INT64, scalar]\n",
      "  %13[FLOAT, 16x32x1x1]\n",
      "  %14[FLOAT, 16]\n",
      "  %15[FLOAT, 16]\n",
      "  %16[FLOAT, 16]\n",
      "  %17[FLOAT, 16]\n",
      "  %18[INT64, scalar]\n",
      "  %19[FLOAT, 32x16x3x3]\n",
      "  %20[FLOAT, 32]\n",
      "  %21[FLOAT, 32]\n",
      "  %22[FLOAT, 32]\n",
      "  %23[FLOAT, 32]\n",
      "  %24[INT64, scalar]\n",
      "  %25[FLOAT, 64x32x3x3]\n",
      "  %26[FLOAT, 64]\n",
      "  %27[FLOAT, 64]\n",
      "  %28[FLOAT, 64]\n",
      "  %29[FLOAT, 64]\n",
      "  %30[INT64, scalar]\n",
      "  %31[FLOAT, 32x64x1x1]\n",
      "  %32[FLOAT, 32]\n",
      "  %33[FLOAT, 32]\n",
      "  %34[FLOAT, 32]\n",
      "  %35[FLOAT, 32]\n",
      "  %36[INT64, scalar]\n",
      "  %37[FLOAT, 64x32x3x3]\n",
      "  %38[FLOAT, 64]\n",
      "  %39[FLOAT, 64]\n",
      "  %40[FLOAT, 64]\n",
      "  %41[FLOAT, 64]\n",
      "  %42[INT64, scalar]\n",
      "  %43[FLOAT, 32x64x1x1]\n",
      "  %44[FLOAT, 32]\n",
      "  %45[FLOAT, 32]\n",
      "  %46[FLOAT, 32]\n",
      "  %47[FLOAT, 32]\n",
      "  %48[INT64, scalar]\n",
      "  %49[FLOAT, 64x32x3x3]\n",
      "  %50[FLOAT, 64]\n",
      "  %51[FLOAT, 64]\n",
      "  %52[FLOAT, 64]\n",
      "  %53[FLOAT, 64]\n",
      "  %54[INT64, scalar]\n",
      "  %55[FLOAT, 128x64x3x3]\n",
      "  %56[FLOAT, 128]\n",
      "  %57[FLOAT, 128]\n",
      "  %58[FLOAT, 128]\n",
      "  %59[FLOAT, 128]\n",
      "  %60[INT64, scalar]\n",
      "  %61[FLOAT, 64x128x1x1]\n",
      "  %62[FLOAT, 64]\n",
      "  %63[FLOAT, 64]\n",
      "  %64[FLOAT, 64]\n",
      "  %65[FLOAT, 64]\n",
      "  %66[INT64, scalar]\n",
      "  %67[FLOAT, 128x64x3x3]\n",
      "  %68[FLOAT, 128]\n",
      "  %69[FLOAT, 128]\n",
      "  %70[FLOAT, 128]\n",
      "  %71[FLOAT, 128]\n",
      "  %72[INT64, scalar]\n",
      "  %73[FLOAT, 10x128]\n",
      "  %74[FLOAT, 10]\n",
      "  %75[FLOAT, 2x10]\n",
      "  %76[FLOAT, 2]\n",
      ") {\n",
      "  %77 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%0, %1)\n",
      "  %78 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 1](%77, %2, %3, %4, %5)\n",
      "  %79 = LeakyRelu[alpha = 0.100000001490116](%78)\n",
      "  %80 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%79, %7)\n",
      "  %81 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 1](%80, %8, %9, %10, %11)\n",
      "  %82 = LeakyRelu[alpha = 0.100000001490116](%81)\n",
      "  %83 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%82, %13)\n",
      "  %84 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 1](%83, %14, %15, %16, %17)\n",
      "  %85 = LeakyRelu[alpha = 0.100000001490116](%84)\n",
      "  %86 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%85, %19)\n",
      "  %87 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 1](%86, %20, %21, %22, %23)\n",
      "  %88 = LeakyRelu[alpha = 0.100000001490116](%87)\n",
      "  %89 = Add(%82, %88)\n",
      "  %90 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%89, %25)\n",
      "  %91 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 1](%90, %26, %27, %28, %29)\n",
      "  %92 = LeakyRelu[alpha = 0.100000001490116](%91)\n",
      "  %93 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%92, %31)\n",
      "  %94 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 1](%93, %32, %33, %34, %35)\n",
      "  %95 = LeakyRelu[alpha = 0.100000001490116](%94)\n",
      "  %96 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%95, %37)\n",
      "  %97 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 1](%96, %38, %39, %40, %41)\n",
      "  %98 = LeakyRelu[alpha = 0.100000001490116](%97)\n",
      "  %99 = Add(%92, %98)\n",
      "  %100 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%99, %43)\n",
      "  %101 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 1](%100, %44, %45, %46, %47)\n",
      "  %102 = LeakyRelu[alpha = 0.100000001490116](%101)\n",
      "  %103 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%102, %49)\n",
      "  %104 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 1](%103, %50, %51, %52, %53)\n",
      "  %105 = LeakyRelu[alpha = 0.100000001490116](%104)\n",
      "  %106 = Add(%99, %105)\n",
      "  %107 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%106, %55)\n",
      "  %108 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 1](%107, %56, %57, %58, %59)\n",
      "  %109 = LeakyRelu[alpha = 0.100000001490116](%108)\n",
      "  %110 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%109, %61)\n",
      "  %111 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 1](%110, %62, %63, %64, %65)\n",
      "  %112 = LeakyRelu[alpha = 0.100000001490116](%111)\n",
      "  %113 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%112, %67)\n",
      "  %114 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 1](%113, %68, %69, %70, %71)\n",
      "  %115 = LeakyRelu[alpha = 0.100000001490116](%114)\n",
      "  %116 = Add(%109, %115)\n",
      "  %117 = GlobalAveragePool(%116)\n",
      "  %118 = Constant[value = <Scalar Tensor []>]()\n",
      "  %119 = Shape(%117)\n",
      "  %120 = Gather[axis = 0](%119, %118)\n",
      "  %121 = Constant[value = <Scalar Tensor []>]()\n",
      "  %122 = Unsqueeze[axes = [0]](%120)\n",
      "  %123 = Unsqueeze[axes = [0]](%121)\n",
      "  %124 = Concat[axis = 0](%122, %123)\n",
      "  %125 = Reshape(%117, %124)\n",
      "  %126 = Gemm[alpha = 1, beta = 1, transB = 1](%125, %73, %74)\n",
      "  %127 = Gemm[alpha = 1, beta = 1, transB = 1](%126, %75, %76)\n",
      "  %128 = Sigmoid(%127)\n",
      "  %129 = Constant[value = <Scalar Tensor []>]()\n",
      "  %130 = Mul(%128, %129)\n",
      "  %131 = Constant[value = <Scalar Tensor []>]()\n",
      "  %132 = Add(%130, %131)\n",
      "  return %132\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(onnx.helper.printable_graph(exported_model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
